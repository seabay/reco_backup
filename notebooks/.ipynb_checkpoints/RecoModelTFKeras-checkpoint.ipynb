{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import builtins\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#from keras import backend as K\n",
    "from extra_keras_metrics import average_precision_at_k\n",
    "from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "import pydot as pyd\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#from keras.layers import Input, Dense, Embedding, Lambda, Reshape, Flatten, Average\n",
    "#from keras.models import Model\n",
    "#from keras.backend import mean, max\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecoDNN():\n",
    "    \n",
    "    def __init__(self, max_transaction_history = 50, max_product_click_history = 50, max_promotion_click_history = 50,\n",
    "                 category_size = 100, single_categorical_features = None, numeric_features_size = 10,\n",
    "                 hidden_layer1_size = 256, hidden_layer2_size = 128, hidden_layer3_size = 64, activation='relu',\n",
    "                input_embedding_size = 128):\n",
    "        \n",
    "        self.max_transaction_history = max_transaction_history\n",
    "        self.max_product_click_history = max_product_click_history\n",
    "        self.max_promotion_click_history = max_promotion_click_history\n",
    "        self.category_size = category_size\n",
    "        self.hidden_layer1_size = hidden_layer1_size\n",
    "        self.hidden_layer2_size = hidden_layer2_size\n",
    "        self.hidden_layer3_size = hidden_layer3_size\n",
    "        self.single_categorical_features = single_categorical_features\n",
    "        self.numeric_features_size = numeric_features_size\n",
    "        self.activation = activation\n",
    "        self.input_embedding_size = input_embedding_size\n",
    "        \n",
    "        self.category_embeddings = tf.keras.layers.Embedding(output_dim=self.input_embedding_size, input_dim = self.category_size, \n",
    "                       input_length = builtins.max(self.max_transaction_history, self.max_product_click_history, self.max_promotion_click_history), mask_zero=True, name='category_embeddings')\n",
    "        \n",
    "        self.build()\n",
    "        \n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        inp_layer, inp_embed = self.create_input()\n",
    "        \n",
    "        v = tf.keras.layers.Dense(self.hidden_layer1_size, activation = self.activation)(tf.keras.layers.concatenate(inp_embed)) \n",
    "        v = tf.keras.layers.Dense(self.hidden_layer2_size, activation = self.activation)(v)\n",
    "        v = tf.keras.layers.Dense(self.hidden_layer3_size, activation = self.activation, name='user_embedding')(v)\n",
    "        output = tf.keras.layers.Dense(self.category_size, activation ='softmax', name='softmax_layer')(v)\n",
    "        self.model = tf.keras.models.Model(inputs = inp_layer, outputs = [output])    \n",
    "        \n",
    "    \n",
    "    def create_input(self):\n",
    "        \n",
    "        transaction_cols = [x for x in range(self.max_transaction_history)]\n",
    "        product_click_cols = [x for x in range(self.max_product_click_history)]\n",
    "        promotion_click_cols = [x for x in range(self.max_promotion_click_history)]\n",
    "        seq_category_cols = [transaction_cols, product_click_cols, promotion_click_cols]\n",
    "        \n",
    "        seqs = []\n",
    "        for i, grp in enumerate(seq_category_cols):\n",
    "            seqs.append(self.seq_categorical_input('seq_categorical_' + str(i), len(grp)))\n",
    "\n",
    "        singles = []\n",
    "        if self.single_categorical_features:\n",
    "            for col in self.single_categorical_features:\n",
    "                singles.append(self.singe_categorical_input(str(col), self.single_categorical_features[col]))\n",
    "\n",
    "        nums = self.continous_inputs(self.numeric_features_size)\n",
    "\n",
    "        inp_layer =  [s[0] for s in seqs]\n",
    "        inp_layer += [s[0] for s in singles]\n",
    "        inp_layer.append(nums[0])\n",
    "        inp_embed = [s[1] for s in seqs]\n",
    "        inp_embed += [s[1] for s in singles]\n",
    "        inp_embed.append(nums[1])\n",
    "               \n",
    "        return inp_layer, inp_embed\n",
    "    \n",
    "    \n",
    "    def seq_categorical_input(self, name, max_history):\n",
    "    \n",
    "        seq = tf.keras.layers.Input(shape=(max_history,), dtype='int32', name=name)\n",
    "        input_embeddings = self.category_embeddings(seq)\n",
    "        #avg = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name= name + '_avg_embedding')\n",
    "        #avg_embedding = tf.reduce_mean(input_embeddings, axis=0), name= name + '_avg_embedding'))\n",
    "        avg_embedding = tf.keras.layers.GlobalAveragePooling1D(name=name + '_avg_embedding')(input_embeddings, mask=self.category_embeddings.compute_mask(seq))\n",
    "\n",
    "\n",
    "        #maxf = Lambda(lambda x: max(x, axis=1), name = name + '_max_embedding')\n",
    "        #max_embedding = maxf(input_embeddings)\n",
    "\n",
    "        return seq, avg_embedding   #keras.layers.add([avg_embedding, max_embedding])\n",
    "\n",
    "    \n",
    "    def singe_categorical_input(self, name, unique_size):\n",
    "        single = tf.keras.layers.Input(shape=(1,), dtype='int32', name=name)\n",
    "        embeddings = tf.keras.layers.Embedding(output_dim = self.input_embedding_size, input_dim = unique_size, \n",
    "                           input_length=1, name=name + '_embedding')(single)\n",
    "        embeddings = tf.keras.layers.Flatten(name = 'flatten_' + name)(embeddings)\n",
    "        return single, embeddings\n",
    "    \n",
    "    def continous_inputs(self, size=None, name='numeric'):\n",
    "        inp = tf.keras.layers.Input(shape=(size,), dtype='float32', name=name)\n",
    "        return inp, inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 10000\n",
    "max_transaction_history = 50\n",
    "max_product_click_history = 50\n",
    "max_promotion_click_history = 50\n",
    "input_embedding_size = 128\n",
    "category_size = 100\n",
    "numeric_size = 10\n",
    "\n",
    "data1 = np.random.randint(category_size, size=(data_size, max_transaction_history-2))\n",
    "data1 = tf.keras.preprocessing.sequence.pad_sequences(data1, 50, padding='post')\n",
    "data2 = np.random.randint(category_size, size=(data_size, max_product_click_history-3))\n",
    "data2 = tf.keras.preprocessing.sequence.pad_sequences(data2, 50, padding='post')\n",
    "\n",
    "data3 = np.random.randint(category_size, size=(data_size, max_promotion_click_history-4))\n",
    "data3 = tf.keras.preprocessing.sequence.pad_sequences(data3, 50, padding='post')\n",
    "\n",
    "inputs = [data1, data2, data3]\n",
    "\n",
    "single_category_cols = {105:3,106:5,107:10}   ## such as location : unique_value_size\n",
    "for k in single_category_cols:\n",
    "    inputs.append(np.random.randint(single_category_cols[k], size=(data_size, 1)))\n",
    "\n",
    "num1 = np.random.random(size=(data_size, numeric_size))\n",
    "inputs.append(num1)\n",
    "\n",
    "labels = np.random.randint(category_size, size=(data_size, 1))\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=category_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46, 49, 23, ..., 52,  0,  0],\n",
       "       [74,  8, 48, ..., 89,  0,  0],\n",
       "       [44, 26, 52, ..., 43,  0,  0],\n",
       "       ...,\n",
       "       [19, 66,  5, ..., 79,  0,  0],\n",
       "       [78, 46, 52, ..., 77,  0,  0],\n",
       "       [33, 66, 71, ..., 65,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs1 = inputs.copy()\n",
    "#inputs1.append(labels)\n",
    "#merge = np.hstack(inputs1)\n",
    "#np.savetxt('../data/features.csv', merge, fmt=\"%2.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tfrecord file for test\n",
    "output_filename = \"../data/tf.tfrecord\"\n",
    "\n",
    "def float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list(value)))\n",
    "\n",
    "with tf.python_io.TFRecordWriter(output_filename) as writer:\n",
    "    for (v1,v2,v3,v4,v5,v6,v7,v8) in zip(data1, data2, data3, inputs[3], inputs[4], inputs[5], inputs[6], labels):\n",
    "        features = {'seq_categorical_0': int64_feature(v1), 'seq_categorical_1': int64_feature(v2),\n",
    "                    'seq_categorical_2': int64_feature(v3),'105': int64_feature(v4),\n",
    "                    '106': int64_feature(v5),'107': int64_feature(v6), 'numeric': float_feature(v7),\n",
    "                    'labels': float_feature(v8)}\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(example_proto):\n",
    "    features = {\"seq_categorical_0\":tf.FixedLenFeature([50],tf.int64),\n",
    "                \"seq_categorical_1\":tf.FixedLenFeature([50], tf.int64),\n",
    "                \"seq_categorical_2\":tf.FixedLenFeature([50], tf.int64),\n",
    "                \"105\":tf.FixedLenFeature([1], tf.int64),\n",
    "                \"106\":tf.FixedLenFeature([1], tf.int64),\n",
    "                \"107\":tf.FixedLenFeature([1], tf.int64),\n",
    "                \"numeric\":tf.FixedLenFeature([10], tf.float32),\n",
    "                \"labels\":tf.FixedLenFeature([1], tf.float32),\n",
    "         }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    #return parsed_features\n",
    "    return (parsed_features[\"seq_categorical_0\"], parsed_features[\"seq_categorical_1\"], parsed_features[\"seq_categorical_2\"], parsed_features[\"105\"], parsed_features[\"106\"], parsed_features[\"107\"], parsed_features[\"numeric\"]), parsed_features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-3b66a34d5f3d>:3: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    }
   ],
   "source": [
    "#debug generated tfrecord\n",
    "try:\n",
    "    for i, str_rec in enumerate(tf.python_io.tf_record_iterator(\"../data/tf.tfrecord\")):\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(str_rec)\n",
    "        #print(i, dict(example.features.feature).keys())\n",
    "except:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3=tf.data.TFRecordDataset('../data/tf.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3=ds3.map(parse_function, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = ds3.repeat()  # Repeat the input indefinitely.\n",
    "ds3 = ds3.batch(32)\n",
    "#iterator = ds3.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorShape([Dimension(None), Dimension(50)]),\n",
       "  TensorShape([Dimension(None), Dimension(50)]),\n",
       "  TensorShape([Dimension(None), Dimension(50)]),\n",
       "  TensorShape([Dimension(None), Dimension(1)]),\n",
       "  TensorShape([Dimension(None), Dimension(1)]),\n",
       "  TensorShape([Dimension(None), Dimension(1)]),\n",
       "  TensorShape([Dimension(None), Dimension(10)])),\n",
       " TensorShape([Dimension(None), Dimension(100)]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds3.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2=tf.data.Dataset.from_tensor_slices(\n",
    "    ({'seq_categorical_0': data1, 'seq_categorical_1': data2, 'seq_categorical_2': data3, \n",
    "     '105': inputs[3], '106': inputs[4],'107': inputs[5],'numeric': inputs[6]},\n",
    "     {'softmax_layer': one_hot_labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2=ds2.repeat()\n",
    "ds2=ds2.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = tf.data.Dataset.from_tensor_slices((data1, data2, data3, inputs[3], inputs[4], inputs[5], num1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(50)]),\n",
       " TensorShape([Dimension(50)]),\n",
       " TensorShape([Dimension(50)]),\n",
       " TensorShape([Dimension(1)]),\n",
       " TensorShape([Dimension(1)]),\n",
       " TensorShape([Dimension(1)]),\n",
       " TensorShape([Dimension(10)]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = tf.data.Dataset.from_tensor_slices((one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100)])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = input_dataset.batch(32).repeat()\n",
    "output_dataset = output_dataset.batch(32).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.zip((input_dataset, output_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.shuffle(100).batch(32).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = ds2.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pengcheng.jia/anaconda3/envs/reco/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = RecoDNN(max_transaction_history,max_product_click_history, max_promotion_click_history, category_size,\n",
    "                numeric_features_size = numeric_size, input_embedding_size = input_embedding_size,\n",
    "                single_categorical_features = single_category_cols).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='../figures/model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pengcheng.jia/anaconda3/envs/reco/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 4.6071 - acc: 0.0119\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 4.6034 - acc: 0.0109\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 4.5984 - acc: 0.0130\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 4.5896 - acc: 0.0144\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 4.5724 - acc: 0.0175\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 4.5425 - acc: 0.0230\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 4.4960 - acc: 0.0292\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 4.4355 - acc: 0.0335\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 4.3591 - acc: 0.0446\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 4.2673 - acc: 0.0538\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 4.1600 - acc: 0.0680\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 4.0339 - acc: 0.0856\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 3.8964 - acc: 0.1040\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.7390 - acc: 0.1288\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 3.5681 - acc: 0.1579\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 3.3916 - acc: 0.1903\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 3.1988 - acc: 0.2252\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 2.9971 - acc: 0.2679\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 2.7996 - acc: 0.3090\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 2.5928 - acc: 0.3548\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 2.3892 - acc: 0.3954\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 2.1941 - acc: 0.4378\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 1.9956 - acc: 0.4928\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 1.7977 - acc: 0.5347\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 1.6169 - acc: 0.5867\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 1.4466 - acc: 0.6239\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 2s 173us/sample - loss: 1.2695 - acc: 0.6731\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 2s 181us/sample - loss: 1.1037 - acc: 0.7250\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 0.9591 - acc: 0.7584\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.8146 - acc: 0.8058\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.6755 - acc: 0.8457\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 0.5536 - acc: 0.8821\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.4573 - acc: 0.9104\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 0.3688 - acc: 0.9322\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 0.2889 - acc: 0.9563\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 0.2249 - acc: 0.9725\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 0.1752 - acc: 0.9797\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 0.1320 - acc: 0.9897\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.0990 - acc: 0.9937\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.0718 - acc: 0.9984\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.0455 - acc: 0.9996\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.0268 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 2s 161us/sample - loss: 0.0180 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 2s 158us/sample - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 0.0045 - acc: 1.0000\n",
      "CPU times: user 3min 11s, sys: 2min 26s, total: 5min 38s\n",
      "Wall time: 1min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1381d2550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(x=inputs, y=one_hot_labels, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pengcheng.jia/anaconda3/envs/reco/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "312/312 [==============================] - 2s 6ms/step - loss: 4.6068 - acc: 0.0084\n",
      "Epoch 2/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.6044 - acc: 0.0118\n",
      "Epoch 3/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5970 - acc: 0.0144\n",
      "Epoch 4/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5873 - acc: 0.0162\n",
      "Epoch 5/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5693 - acc: 0.0178\n",
      "Epoch 6/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5430 - acc: 0.0233\n",
      "Epoch 7/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5103 - acc: 0.0254\n",
      "Epoch 8/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.4684 - acc: 0.0301\n",
      "Epoch 9/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.4245 - acc: 0.0359\n",
      "Epoch 10/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.3833 - acc: 0.0388\n",
      "Epoch 11/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 4.3407 - acc: 0.0420\n",
      "Epoch 12/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 4.3014 - acc: 0.0486\n",
      "Epoch 13/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.2601 - acc: 0.0522\n",
      "Epoch 14/50\n",
      "312/312 [==============================] - 2s 6ms/step - loss: 4.2213 - acc: 0.0564\n",
      "Epoch 15/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.1800 - acc: 0.0607\n",
      "Epoch 16/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 4.1395 - acc: 0.0656\n",
      "Epoch 17/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.0987 - acc: 0.0723\n",
      "Epoch 18/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.0601 - acc: 0.0757\n",
      "Epoch 19/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 4.0129 - acc: 0.0830\n",
      "Epoch 20/50\n",
      "312/312 [==============================] - 1s 4ms/step - loss: 3.9589 - acc: 0.0892\n",
      "Epoch 21/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.9069 - acc: 0.1007\n",
      "Epoch 22/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.8467 - acc: 0.1082\n",
      "Epoch 23/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 3.7878 - acc: 0.1180\n",
      "Epoch 24/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 3.7281 - acc: 0.1263\n",
      "Epoch 25/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.6599 - acc: 0.1389\n",
      "Epoch 26/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.5977 - acc: 0.1517\n",
      "Epoch 27/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.5314 - acc: 0.1605\n",
      "Epoch 28/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 3.4606 - acc: 0.1716\n",
      "Epoch 29/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.3947 - acc: 0.1855\n",
      "Epoch 30/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 3.3201 - acc: 0.2014\n",
      "Epoch 31/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.2515 - acc: 0.2117\n",
      "Epoch 32/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.1815 - acc: 0.2270\n",
      "Epoch 33/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.1080 - acc: 0.2383\n",
      "Epoch 34/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 3.0422 - acc: 0.2500\n",
      "Epoch 35/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.9700 - acc: 0.2732\n",
      "Epoch 36/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.9034 - acc: 0.2786\n",
      "Epoch 37/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.8415 - acc: 0.2875\n",
      "Epoch 38/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.7745 - acc: 0.3052\n",
      "Epoch 39/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.7029 - acc: 0.3220\n",
      "Epoch 40/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.6218 - acc: 0.3373\n",
      "Epoch 41/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.5419 - acc: 0.3568\n",
      "Epoch 42/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.4834 - acc: 0.3622\n",
      "Epoch 43/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.4120 - acc: 0.3856\n",
      "Epoch 44/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.3447 - acc: 0.3998\n",
      "Epoch 45/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.2749 - acc: 0.4128\n",
      "Epoch 46/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.1923 - acc: 0.4369\n",
      "Epoch 47/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.1264 - acc: 0.4500\n",
      "Epoch 48/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.0492 - acc: 0.4676\n",
      "Epoch 49/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 1.9913 - acc: 0.4768\n",
      "Epoch 50/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.9328 - acc: 0.4846\n",
      "CPU times: user 3min 21s, sys: 2min 9s, total: 5min 30s\n",
      "Wall time: 1min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x130cd2f60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%time model.fit(ds3, epochs=50, steps_per_epoch=int(10000//32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=data_generator.data_generator('../data/features.csv', 64, [50, 100, 150], [150,151,152], [153])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator=data_generator.DataGenerator('../data/features.csv', 10000, 64, 100, [50, 100, 150], [150,151,152], [153])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit_generator(generator,steps_per_epoch=10000//64, verbose=1, epochs=50, shuffle=True, use_multiprocessing=True, workers=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
