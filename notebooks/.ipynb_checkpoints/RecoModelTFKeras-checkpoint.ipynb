{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import builtins\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#from keras import backend as K\n",
    "#from extra_keras_metrics import average_precision_at_k\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "#import pydot as pyd\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#from keras.layers import Input, Dense, Embedding, Lambda, Reshape, Flatten, Average\n",
    "#from keras.models import Model\n",
    "#from keras.backend import mean, max\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#keras.utils.vis_utils.pydot = pyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecoDNN():\n",
    "    \n",
    "    def __init__(self, max_transaction_history = 50, max_product_click_history = 50, max_promotion_click_history = 50,\n",
    "                 category_size = 100, single_categorical_features = None, numeric_features_size = 10,\n",
    "                 hidden_layer1_size = 256, hidden_layer2_size = 128, hidden_layer3_size = 64, activation='relu',\n",
    "                input_embedding_size = 128):\n",
    "        \n",
    "        self.max_transaction_history = max_transaction_history\n",
    "        self.max_product_click_history = max_product_click_history\n",
    "        self.max_promotion_click_history = max_promotion_click_history\n",
    "        self.category_size = category_size\n",
    "        self.hidden_layer1_size = hidden_layer1_size\n",
    "        self.hidden_layer2_size = hidden_layer2_size\n",
    "        self.hidden_layer3_size = hidden_layer3_size\n",
    "        self.single_categorical_features = single_categorical_features\n",
    "        self.numeric_features_size = numeric_features_size\n",
    "        self.activation = activation\n",
    "        self.input_embedding_size = input_embedding_size\n",
    "        \n",
    "        self.category_embeddings = tf.keras.layers.Embedding(output_dim=self.input_embedding_size, input_dim = self.category_size, \n",
    "                       input_length = builtins.max(self.max_transaction_history, self.max_product_click_history, self.max_promotion_click_history), mask_zero=True, name='category_embeddings')\n",
    "        \n",
    "        self.build()\n",
    "        \n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        inp_layer, inp_embed = self.create_input()\n",
    "        \n",
    "        v = tf.keras.layers.Dense(self.hidden_layer1_size, activation = self.activation)(tf.keras.layers.concatenate(inp_embed)) \n",
    "        v = tf.keras.layers.Dense(self.hidden_layer2_size, activation = self.activation)(v)\n",
    "        v = tf.keras.layers.Dense(self.hidden_layer3_size, activation = self.activation, name='user_embedding')(v)\n",
    "        output = tf.keras.layers.Dense(self.category_size, activation ='softmax', name='softmax_layer')(v)\n",
    "        self.model = tf.keras.models.Model(inputs = inp_layer, outputs = [output])    \n",
    "        \n",
    "    \n",
    "    def create_input(self):\n",
    "        \n",
    "        transaction_cols = [x for x in range(self.max_transaction_history)]\n",
    "        product_click_cols = [x for x in range(self.max_product_click_history)]\n",
    "        promotion_click_cols = [x for x in range(self.max_promotion_click_history)]\n",
    "        seq_category_cols = [transaction_cols, product_click_cols, promotion_click_cols]\n",
    "        \n",
    "        seqs = []\n",
    "        for i, grp in enumerate(seq_category_cols):\n",
    "            seqs.append(self.seq_categorical_input('seq_categorical_' + str(i), len(grp)))\n",
    "\n",
    "        singles = []\n",
    "        if self.single_categorical_features:\n",
    "            for col in self.single_categorical_features:\n",
    "                singles.append(self.singe_categorical_input(str(col), self.single_categorical_features[col]))\n",
    "\n",
    "        nums = self.continous_inputs(self.numeric_features_size)\n",
    "\n",
    "        inp_layer =  [s[0] for s in seqs]\n",
    "        inp_layer += [s[0] for s in singles]\n",
    "        inp_layer.append(nums[0])\n",
    "        inp_embed = [s[1] for s in seqs]\n",
    "        inp_embed += [s[1] for s in singles]\n",
    "        inp_embed.append(nums[1])\n",
    "               \n",
    "        return inp_layer, inp_embed\n",
    "    \n",
    "    \n",
    "    def seq_categorical_input(self, name, max_history):\n",
    "    \n",
    "        seq = tf.keras.layers.Input(shape=(max_history,), dtype='int32', name=name)\n",
    "        input_embeddings = self.category_embeddings(seq)\n",
    "        avg = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name= name + '_avg_embedding')\n",
    "        #avg_embedding = tf.reduce_mean(input_embeddings, axis=0), name= name + '_avg_embedding'))\n",
    "        avg_embedding = avg(input_embeddings)\n",
    "\n",
    "        #maxf = Lambda(lambda x: max(x, axis=1), name = name + '_max_embedding')\n",
    "        #max_embedding = maxf(input_embeddings)\n",
    "\n",
    "        return seq, avg_embedding   #keras.layers.add([avg_embedding, max_embedding])\n",
    "\n",
    "    \n",
    "    def singe_categorical_input(self, name, unique_size):\n",
    "        single = tf.keras.layers.Input(shape=(1,), dtype='int32', name=name)\n",
    "        embeddings = tf.keras.layers.Embedding(output_dim = self.input_embedding_size, input_dim = unique_size, \n",
    "                           input_length=1, name=name + '_embedding')(single)\n",
    "        embeddings = tf.keras.layers.Flatten(name = 'flatten_' + name)(embeddings)\n",
    "        return single, embeddings\n",
    "    \n",
    "    def continous_inputs(self, size=None, name='numeric'):\n",
    "        inp = tf.keras.layers.Input(shape=(size,), dtype='float32', name=name)\n",
    "        return inp, inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 10000\n",
    "max_transaction_history = 50\n",
    "max_product_click_history = 50\n",
    "max_promotion_click_history = 50\n",
    "input_embedding_size = 128\n",
    "category_size = 100\n",
    "numeric_size = 10\n",
    "\n",
    "data1 = np.random.randint(category_size, size=(data_size, max_transaction_history))\n",
    "data2 = np.random.randint(category_size, size=(data_size, max_product_click_history))\n",
    "data3 = np.random.randint(category_size, size=(data_size, max_promotion_click_history))\n",
    "inputs = [data1, data2, data3]\n",
    "\n",
    "single_category_cols = {105:3,106:5,107:10}   ## such as location : unique_value_size\n",
    "for k in single_category_cols:\n",
    "    inputs.append(np.random.randint(single_category_cols[k], size=(data_size, 1)))\n",
    "\n",
    "num1 = np.random.random(size=(data_size, numeric_size))\n",
    "inputs.append(num1)\n",
    "\n",
    "labels = np.random.randint(category_size, size=(data_size, 1))\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=category_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = inputs.copy()\n",
    "inputs1.append(labels)\n",
    "merge = np.hstack(inputs1)\n",
    "np.savetxt('../data/features.csv', merge, fmt=\"%2.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tfrecord file for test\n",
    "output_filename = \"../data/tf.tfrecord\"\n",
    "\n",
    "def float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list(value)))\n",
    "\n",
    "with tf.python_io.TFRecordWriter(output_filename) as writer:\n",
    "    for (v1,v2,v3,v4,v5,v6,v7,v8) in zip(data1, data2, data3, inputs[3], inputs[4], inputs[5], inputs[6], one_hot_labels):\n",
    "        features = {'seq_categorical_0': int64_feature(v1), 'seq_categorical_1': int64_feature(v2),\n",
    "                    'seq_categorical_2': int64_feature(v3),'105': int64_feature(v4),\n",
    "                    '106': int64_feature(v5),'107': int64_feature(v6), 'numeric': float_feature(v7),\n",
    "                    'labels': float_feature(v8)}\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(example_proto):\n",
    "    features = {\"seq_categorical_0\":tf.FixedLenFeature([50],tf.int64),\n",
    "                \"seq_categorical_1\":tf.FixedLenFeature([50], tf.int64),\n",
    "                \"seq_categorical_2\":tf.FixedLenFeature([50], tf.int64),\n",
    "                \"105\":tf.FixedLenFeature([1], tf.int64),\n",
    "                \"106\":tf.FixedLenFeature([1], tf.int64),\n",
    "                \"107\":tf.FixedLenFeature([1], tf.int64),\n",
    "                \"numeric\":tf.FixedLenFeature([10], tf.float32),\n",
    "                \"labels\":tf.FixedLenFeature([100], tf.float32),\n",
    "         }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    #return parsed_features\n",
    "    return (parsed_features[\"seq_categorical_0\"], parsed_features[\"seq_categorical_1\"], parsed_features[\"seq_categorical_2\"], parsed_features[\"105\"], parsed_features[\"106\"], parsed_features[\"107\"], parsed_features[\"numeric\"]), parsed_features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug generated tfrecord\n",
    "try:\n",
    "    for i, str_rec in enumerate(tf.python_io.tf_record_iterator(\"../data/tf.tfrecord\")):\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(str_rec)\n",
    "        #print(i, dict(example.features.feature).keys())\n",
    "except:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3=tf.data.TFRecordDataset('../data/tf.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3=ds3.map(parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = ds3.repeat()  # Repeat the input indefinitely.\n",
    "ds3 = ds3.batch(32)\n",
    "#iterator = ds3.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorShape([Dimension(None), Dimension(50)]),\n",
       "  TensorShape([Dimension(None), Dimension(50)]),\n",
       "  TensorShape([Dimension(None), Dimension(50)]),\n",
       "  TensorShape([Dimension(None), Dimension(1)]),\n",
       "  TensorShape([Dimension(None), Dimension(1)]),\n",
       "  TensorShape([Dimension(None), Dimension(1)]),\n",
       "  TensorShape([Dimension(None), Dimension(10)])),\n",
       " TensorShape([Dimension(None), Dimension(100)]))"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds3.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2=tf.data.Dataset.from_tensor_slices(\n",
    "    ({'seq_categorical_0': data1, 'seq_categorical_1': data2, 'seq_categorical_2': data3, \n",
    "     '105': inputs[3], '106': inputs[4],'107': inputs[5],'numeric': inputs[6]},\n",
    "     {'softmax_layer': one_hot_labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2=ds2.repeat()\n",
    "ds2=ds2.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = tf.data.Dataset.from_tensor_slices((data1, data2, data3, inputs[3], inputs[4], inputs[5], num1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(50)]),\n",
       " TensorShape([Dimension(50)]),\n",
       " TensorShape([Dimension(50)]),\n",
       " TensorShape([Dimension(1)]),\n",
       " TensorShape([Dimension(1)]),\n",
       " TensorShape([Dimension(1)]),\n",
       " TensorShape([Dimension(10)]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = tf.data.Dataset.from_tensor_slices((one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100)])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = input_dataset.batch(32).repeat()\n",
    "output_dataset = output_dataset.batch(32).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.zip((input_dataset, output_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.shuffle(100).batch(32).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = ds2.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecoDNN(max_transaction_history,max_product_click_history, max_promotion_click_history, category_size,\n",
    "                numeric_features_size = numeric_size, input_embedding_size = input_embedding_size,\n",
    "                single_categorical_features = single_category_cols).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='../figures/model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1752a1e5a8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SVG' is not defined"
     ]
    }
   ],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time model.fit(x=inputs, y=one_hot_labels, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "312/312 [==============================] - 3s 10ms/step - loss: 4.5935 - acc: 0.0149\n",
      "Epoch 2/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5767 - acc: 0.0164\n",
      "Epoch 3/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5484 - acc: 0.0194\n",
      "Epoch 4/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.5083 - acc: 0.0242\n",
      "Epoch 5/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.4582 - acc: 0.0279\n",
      "Epoch 6/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.4038 - acc: 0.0341\n",
      "Epoch 7/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.3475 - acc: 0.0425\n",
      "Epoch 8/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.2938 - acc: 0.0478\n",
      "Epoch 9/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.2428 - acc: 0.0539\n",
      "Epoch 10/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.1943 - acc: 0.0594\n",
      "Epoch 11/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.1410 - acc: 0.0660\n",
      "Epoch 12/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.0857 - acc: 0.0714\n",
      "Epoch 13/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 4.0280 - acc: 0.0809\n",
      "Epoch 14/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.9717 - acc: 0.0895\n",
      "Epoch 15/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.9112 - acc: 0.0994\n",
      "Epoch 16/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.8558 - acc: 0.1057\n",
      "Epoch 17/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.7878 - acc: 0.1211\n",
      "Epoch 18/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.7244 - acc: 0.1279\n",
      "Epoch 19/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.6615 - acc: 0.1406\n",
      "Epoch 20/50\n",
      "312/312 [==============================] - 2s 6ms/step - loss: 3.5974 - acc: 0.1485\n",
      "Epoch 21/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.5336 - acc: 0.1574\n",
      "Epoch 22/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.4700 - acc: 0.1701\n",
      "Epoch 23/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.4078 - acc: 0.1851\n",
      "Epoch 24/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.3382 - acc: 0.1953\n",
      "Epoch 25/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.2631 - acc: 0.2090\n",
      "Epoch 26/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 3.1854 - acc: 0.2228\n",
      "Epoch 27/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.0980 - acc: 0.2395\n",
      "Epoch 28/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 3.0131 - acc: 0.2605\n",
      "Epoch 29/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.9338 - acc: 0.2731\n",
      "Epoch 30/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.8579 - acc: 0.2894\n",
      "Epoch 31/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.7769 - acc: 0.3025\n",
      "Epoch 32/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.6820 - acc: 0.3216\n",
      "Epoch 33/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.6036 - acc: 0.3381\n",
      "Epoch 34/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.5236 - acc: 0.3529\n",
      "Epoch 35/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.4449 - acc: 0.3728\n",
      "Epoch 36/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.3671 - acc: 0.3877\n",
      "Epoch 37/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.2761 - acc: 0.4101\n",
      "Epoch 38/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.1928 - acc: 0.4313\n",
      "Epoch 39/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 2.1178 - acc: 0.4464\n",
      "Epoch 40/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 2.0376 - acc: 0.4649\n",
      "Epoch 41/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.9631 - acc: 0.4811\n",
      "Epoch 42/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.8777 - acc: 0.5040\n",
      "Epoch 43/50\n",
      "312/312 [==============================] - 2s 6ms/step - loss: 1.8058 - acc: 0.5189\n",
      "Epoch 44/50\n",
      "312/312 [==============================] - 2s 6ms/step - loss: 1.7330 - acc: 0.5431\n",
      "Epoch 45/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.6616 - acc: 0.5601\n",
      "Epoch 46/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.5989 - acc: 0.5738\n",
      "Epoch 47/50\n",
      "312/312 [==============================] - 1s 5ms/step - loss: 1.5268 - acc: 0.5965\n",
      "Epoch 48/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.4623 - acc: 0.6065\n",
      "Epoch 49/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.4058 - acc: 0.6228\n",
      "Epoch 50/50\n",
      "312/312 [==============================] - 2s 5ms/step - loss: 1.3530 - acc: 0.6315\n",
      "CPU times: user 3min 5s, sys: 2min 10s, total: 5min 16s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x315088ba8>"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(ds3.make_one_shot_iterator(), epochs=50, steps_per_epoch=int(10000//32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=data_generator.data_generator('../data/features.csv', 64, [50, 100, 150], [150,151,152], [153])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator=data_generator.DataGenerator('../data/features.csv', 10000, 64, 100, [50, 100, 150], [150,151,152], [153])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit_generator(generator,steps_per_epoch=10000//64, verbose=1, epochs=50, shuffle=True, use_multiprocessing=True, workers=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
